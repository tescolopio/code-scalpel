#!/usr/bin/env python3
"""
Code Scalpel Vulnerability Detection Benchmark Runner
======================================================

Runs all 120 test cases through Code Scalpel's security analyzer
and generates detailed results with metrics.

Usage:
    python run_benchmark.py [--output results.json] [--verbose]
"""

import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from test_cases import (
    ALL_VULNERABILITY_CATEGORIES,
    get_all_test_cases,
    get_vulnerable_cases,
    get_safe_cases,
    get_category_summary
)

from src.code_scalpel.code_analyzer import CodeAnalyzer
from src.code_scalpel.symbolic_execution_tools.security_analyzer import SecurityAnalyzer


class BenchmarkRunner:
    """Runs vulnerability detection benchmarks."""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.results: List[Dict[str, Any]] = []
        self.security_analyzer = SecurityAnalyzer()

    def log(self, message: str):
        """Print if verbose mode enabled."""
        if self.verbose:
            print(message)

    def run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Run a single test case and return results."""
        start_time = time.time()

        result = {
            "id": test_case["id"],
            "cwe_id": test_case["cwe_id"],
            "category": test_case["category_name"],
            "name": test_case["name"],
            "expected_vulnerable": test_case["vulnerable"],
            "expected_line": test_case.get("expected_line"),
            "actual_vulnerable": False,
            "actual_findings": [],
            "correct": False,
            "execution_time_ms": 0,
            "error": None
        }

        try:
            # Create a temporary analyzer for this code
            code = test_case["code"].strip()

            # Run security analysis using the dedicated SecurityAnalyzer
            analysis_result = self.security_analyzer.analyze(code)

            # Check if vulnerabilities were found
            vulnerabilities = analysis_result.vulnerabilities if analysis_result else []

            result["actual_vulnerable"] = len(vulnerabilities) > 0
            result["actual_findings"] = [
                {
                    "type": getattr(v, "vulnerability_type", "unknown"),
                    "line": getattr(v, "line_number", None),
                    "message": getattr(v, "description", ""),
                    "severity": getattr(v, "severity", "unknown"),
                    "cwe": getattr(v, "cwe_id", None)
                }
                for v in vulnerabilities
            ]

            # Determine correctness
            if test_case["vulnerable"]:
                # Should have found vulnerability
                result["correct"] = len(vulnerabilities) > 0
            else:
                # Should NOT have found vulnerability
                result["correct"] = len(vulnerabilities) == 0

        except Exception as e:
            result["error"] = str(e)
            result["correct"] = False

        result["execution_time_ms"] = round((time.time() - start_time) * 1000, 2)
        return result

    def run_all_tests(self) -> Dict[str, Any]:
        """Run all test cases and return aggregated results."""
        all_cases = get_all_test_cases()
        total = len(all_cases)

        print(f"\n{'='*70}")
        print("RUNNING CODE SCALPEL VULNERABILITY DETECTION BENCHMARK")
        print(f"{'='*70}")
        print(f"Total test cases: {total}")
        print(f"Started: {datetime.now().isoformat()}")
        print()

        self.results = []
        start_time = time.time()

        for i, test_case in enumerate(all_cases, 1):
            self.log(f"[{i}/{total}] Testing {test_case['id']}: {test_case['name']}")

            result = self.run_single_test(test_case)
            self.results.append(result)

            status = "PASS" if result["correct"] else "FAIL"
            self.log(f"         -> {status}")

            # Progress indicator for non-verbose mode
            if not self.verbose and i % 10 == 0:
                print(f"Progress: {i}/{total} ({i*100//total}%)")

        total_time = time.time() - start_time

        return self.generate_report(total_time)

    def generate_report(self, total_time: float) -> Dict[str, Any]:
        """Generate comprehensive benchmark report."""

        # Overall metrics
        total = len(self.results)
        correct = sum(1 for r in self.results if r["correct"])
        incorrect = total - correct

        # Split by expected vulnerability
        vulnerable_cases = [r for r in self.results if r["expected_vulnerable"]]
        safe_cases = [r for r in self.results if not r["expected_vulnerable"]]

        # True/false positives and negatives
        true_positives = sum(1 for r in vulnerable_cases if r["actual_vulnerable"])
        false_negatives = sum(1 for r in vulnerable_cases if not r["actual_vulnerable"])
        true_negatives = sum(1 for r in safe_cases if not r["actual_vulnerable"])
        false_positives = sum(1 for r in safe_cases if r["actual_vulnerable"])

        # Calculate metrics
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = correct / total if total > 0 else 0

        # Per-category breakdown
        category_results = {}
        for category in ALL_VULNERABILITY_CATEGORIES:
            cwe_id = category["cwe_id"]
            cat_results = [r for r in self.results if r["cwe_id"] == cwe_id]

            cat_vuln = [r for r in cat_results if r["expected_vulnerable"]]
            cat_safe = [r for r in cat_results if not r["expected_vulnerable"]]

            cat_tp = sum(1 for r in cat_vuln if r["actual_vulnerable"])
            cat_fn = sum(1 for r in cat_vuln if not r["actual_vulnerable"])
            cat_tn = sum(1 for r in cat_safe if not r["actual_vulnerable"])
            cat_fp = sum(1 for r in cat_safe if r["actual_vulnerable"])

            category_results[cwe_id] = {
                "name": category["name"],
                "total": len(cat_results),
                "correct": cat_tp + cat_tn,
                "true_positives": cat_tp,
                "false_negatives": cat_fn,
                "true_negatives": cat_tn,
                "false_positives": cat_fp,
                "detection_rate": cat_tp / len(cat_vuln) * 100 if cat_vuln else 100,
                "false_positive_rate": cat_fp / len(cat_safe) * 100 if cat_safe else 0
            }

        # Failed test details
        failed_tests = [
            {
                "id": r["id"],
                "cwe_id": r["cwe_id"],
                "name": r["name"],
                "expected": "vulnerable" if r["expected_vulnerable"] else "safe",
                "actual": "vulnerable" if r["actual_vulnerable"] else "safe",
                "findings": r["actual_findings"]
            }
            for r in self.results if not r["correct"]
        ]

        report = {
            "benchmark_info": {
                "name": "Code Scalpel Vulnerability Detection Benchmark",
                "version": "1.0.0",
                "timestamp": datetime.now().isoformat(),
                "total_execution_time_seconds": round(total_time, 2),
                "average_test_time_ms": round(sum(r["execution_time_ms"] for r in self.results) / total, 2)
            },
            "summary": {
                "total_test_cases": total,
                "correct": correct,
                "incorrect": incorrect,
                "accuracy_percentage": round(accuracy * 100, 2),
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "f1_score": round(f1_score, 4)
            },
            "confusion_matrix": {
                "true_positives": true_positives,
                "false_positives": false_positives,
                "true_negatives": true_negatives,
                "false_negatives": false_negatives
            },
            "detection_metrics": {
                "vulnerable_cases_total": len(vulnerable_cases),
                "vulnerable_cases_detected": true_positives,
                "detection_rate_percentage": round(true_positives / len(vulnerable_cases) * 100, 2) if vulnerable_cases else 100,
                "safe_cases_total": len(safe_cases),
                "safe_cases_correct": true_negatives,
                "false_positive_rate_percentage": round(false_positives / len(safe_cases) * 100, 2) if safe_cases else 0
            },
            "category_breakdown": category_results,
            "failed_tests": failed_tests,
            "all_results": self.results
        }

        return report

    def print_summary(self, report: Dict[str, Any]):
        """Print human-readable summary."""
        print(f"\n{'='*70}")
        print("BENCHMARK RESULTS SUMMARY")
        print(f"{'='*70}\n")

        summary = report["summary"]
        print(f"Total Test Cases:     {summary['total_test_cases']}")
        print(f"Correct:              {summary['correct']}")
        print(f"Incorrect:            {summary['incorrect']}")
        print(f"Accuracy:             {summary['accuracy_percentage']}%")
        print(f"Precision:            {summary['precision']}")
        print(f"Recall:               {summary['recall']}")
        print(f"F1 Score:             {summary['f1_score']}")

        print(f"\n{'='*70}")
        print("CONFUSION MATRIX")
        print(f"{'='*70}\n")

        cm = report["confusion_matrix"]
        print(f"                    Predicted Vulnerable    Predicted Safe")
        print(f"Actually Vulnerable       {cm['true_positives']:>6}                {cm['false_negatives']:>6}")
        print(f"Actually Safe             {cm['false_positives']:>6}                {cm['true_negatives']:>6}")

        print(f"\n{'='*70}")
        print("DETECTION METRICS")
        print(f"{'='*70}\n")

        dm = report["detection_metrics"]
        print(f"Vulnerable cases detected: {dm['vulnerable_cases_detected']}/{dm['vulnerable_cases_total']} ({dm['detection_rate_percentage']}%)")
        print(f"False positive rate:       {dm['false_positive_rate_percentage']}%")

        print(f"\n{'='*70}")
        print("PER-CATEGORY BREAKDOWN")
        print(f"{'='*70}\n")

        print(f"{'CWE ID':<10} {'Category':<30} {'Detection%':<12} {'FP Rate%':<10} {'TP/FN/TN/FP'}")
        print("-" * 80)

        for cwe_id, cat in report["category_breakdown"].items():
            tp_fn_tn_fp = f"{cat['true_positives']}/{cat['false_negatives']}/{cat['true_negatives']}/{cat['false_positives']}"
            print(f"{cwe_id:<10} {cat['name']:<30} {cat['detection_rate']:>10.1f}% {cat['false_positive_rate']:>9.1f}% {tp_fn_tn_fp}")

        if report["failed_tests"]:
            print(f"\n{'='*70}")
            print("FAILED TESTS")
            print(f"{'='*70}\n")

            for test in report["failed_tests"][:10]:  # Show first 10 failures
                print(f"  {test['id']}: {test['name']}")
                print(f"    Expected: {test['expected']}, Got: {test['actual']}")

            if len(report["failed_tests"]) > 10:
                print(f"\n  ... and {len(report['failed_tests']) - 10} more failures")

        print(f"\n{'='*70}")
        print(f"Benchmark completed in {report['benchmark_info']['total_execution_time_seconds']} seconds")
        print(f"Average test time: {report['benchmark_info']['average_test_time_ms']} ms")
        print(f"{'='*70}\n")


def main():
    parser = argparse.ArgumentParser(description="Run Code Scalpel vulnerability detection benchmark")
    parser.add_argument("--output", "-o", default="results.json", help="Output file for JSON results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    args = parser.parse_args()

    runner = BenchmarkRunner(verbose=args.verbose)
    report = runner.run_all_tests()
    runner.print_summary(report)

    # Save results
    output_path = Path(__file__).parent / args.output
    with open(output_path, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"Detailed results saved to: {output_path}")

    # Return exit code based on results
    # Consider it a pass if accuracy >= 90%
    if report["summary"]["accuracy_percentage"] >= 90:
        return 0
    return 1


if __name__ == "__main__":
    sys.exit(main())
